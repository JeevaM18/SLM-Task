# ğŸ§  Model Report - Multilingual Sentiment Classification

## ğŸ“Œ Overview

This report summarizes the design decisions, training process, evaluation outcomes, and possible improvements for the multilingual sentiment classification model developed using the `bert-base-multilingual-cased` transformer.

---

## ğŸ—ï¸ Model Architecture

* **Base Model**: `bert-base-multilingual-cased` from Hugging Face
* **Model Type**: Transformer-based Sequence Classification
* **Fine-tuning Head**: Linear classification layer on top of pooled output
* **Number of Labels**: 3 (Negative, Neutral, Positive)

---

## ğŸ› ï¸ Training Configuration

* **Tokenizer**: AutoTokenizer from pre-trained checkpoint
* **Model**: AutoModelForSequenceClassification
* **Training Arguments**:

  * Epochs: 3
  * Learning Rate: 2e-5
  * Batch Size: 16
  * Weight Decay: 0.01
  * Save Total Limit: 1
  * Logging Strategy: per epoch

---

## ğŸ“Š Dataset

* **Source**: `tyqiangz/multilingual-sentiments`
* **Languages**: English, Spanish, German
* **Data Size**:

  * Train: 2,100
  * Validation: 450
  * Test: 450

---

## âœ… Performance Metrics

| Metric    | Score |
| --------- | ----- |
| Accuracy  | 0.602 |
| Precision | 0.615 |
| Recall    | 0.602 |
| F1 Score  | 0.603 |
| Eval Loss | 0.996 |

---

## ğŸ“‰ Error Analysis

* **Observation**: Most confusion occurs between neutral and positive samples.
* **Short Texts**: Single-word inputs led to ambiguous predictions.
* **Language Influence**: Slight performance dip on German samples.

---

## ğŸ”§ Improvements

* âœ… Use of XLM-Roberta instead of mBERT for better multilingual alignment
* âœ… Increase training epochs (5â€“10) for better convergence
* âœ… Perform language-specific fine-tuning
* âœ… Apply advanced data augmentation techniques (e.g., back translation)

---

## ğŸ—‚ï¸ Artifacts Saved

* `models/trained_model/` â€“ Final model checkpoint
* `models/tokenizer/` â€“ Corresponding tokenizer files
* `reports/evaluation_metrics.json` â€“ Metrics dump
* `reports/confusion_matrix.png` â€“ Visualization

---

## ğŸ“Œ Summary

The fine-tuned multilingual BERT model achieves over 60% accuracy and balanced F1 score across three languages on sentiment classification. Further tuning and model enhancements can lead to significantly improved cross-lingual generalization and robustness.

---

